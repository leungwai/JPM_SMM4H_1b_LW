{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/epoch_12/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/epoch_12/epoch_13/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/epoch_12/epoch_13/epoch_14/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/epoch_12/epoch_13/epoch_14/epoch_15/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/epoch_12/epoch_13/epoch_14/epoch_15/epoch_16/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/epoch_12/epoch_13/epoch_14/epoch_15/epoch_16/epoch_17/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/epoch_12/epoch_13/epoch_14/epoch_15/epoch_16/epoch_17/epoch_18/unformatted_result.tsv\n",
      "bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/epoch_0/epoch_1/epoch_2/epoch_3/epoch_4/epoch_5/epoch_6/epoch_7/epoch_8/epoch_9/epoch_10/epoch_11/epoch_12/epoch_13/epoch_14/epoch_15/epoch_16/epoch_17/epoch_18/epoch_19/unformatted_result.tsv\n"
     ]
    }
   ],
   "source": [
    "list_of_df = []\n",
    "n_epochs = 20\n",
    "\n",
    "epoch_string = 'bert-base/20_epochs_baseline/saved_report_1b/bert-base-uncased/1/'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_string += 'epoch_' + str(epoch) + '/'\n",
    "    epoch_location = epoch_string + 'unformatted_result.tsv'\n",
    "    print(epoch_location)\n",
    "    list_of_df.append(pd.read_csv(epoch_location, sep='\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>class</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>span</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SMM4H2022ykI8vN7jZYnV57AM</td>\n",
       "      <td>ADE</td>\n",
       "      <td>119</td>\n",
       "      <td>125</td>\n",
       "      <td>nerves</td>\n",
       "      <td>@USER_________ i found the humira to fix all m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SMM4H2022ykI8vN7jZYnV57AM</td>\n",
       "      <td>ADE</td>\n",
       "      <td>126</td>\n",
       "      <td>139</td>\n",
       "      <td>muscle spasms</td>\n",
       "      <td>@USER_________ i found the humira to fix all m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMM4H2022uCZV2SRsCe4vzjFm</td>\n",
       "      <td>ADE</td>\n",
       "      <td>61</td>\n",
       "      <td>68</td>\n",
       "      <td>gaining</td>\n",
       "      <td>@USER__________ have to go to a doc now to see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMM4H2022uCZV2SRsCe4vzjFm</td>\n",
       "      <td>ADE</td>\n",
       "      <td>91</td>\n",
       "      <td>110</td>\n",
       "      <td>gain like 50 pounds</td>\n",
       "      <td>@USER__________ have to go to a doc now to see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SMM4H20229Aha6m4XERqYdFWf</td>\n",
       "      <td>ADE</td>\n",
       "      <td>118</td>\n",
       "      <td>134</td>\n",
       "      <td>frontal headache</td>\n",
       "      <td>06.30 day 14 Rivaroxaban diary. Thanks to para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>SMM4H2022cjwbGQbnkpVjjJzR</td>\n",
       "      <td>ADE</td>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td>cotton mouth</td>\n",
       "      <td>Fucking Vyvanse, giving me cotton mouth. UGH.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>SMM4H2022lX1A8RVuySkDzAB2</td>\n",
       "      <td>ADE</td>\n",
       "      <td>84</td>\n",
       "      <td>104</td>\n",
       "      <td>couldn't fall asleep</td>\n",
       "      <td>between the fucking redbull and vyvanse i popp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>SMM4H2022qZC2BPG2BW7UC175</td>\n",
       "      <td>ADE</td>\n",
       "      <td>45</td>\n",
       "      <td>48</td>\n",
       "      <td>OCD</td>\n",
       "      <td>rt @USER_______: vyvanse, commonly known as oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>SMM4H20226b5WyZPTAJ7qL3dE</td>\n",
       "      <td>ADE</td>\n",
       "      <td>63</td>\n",
       "      <td>71</td>\n",
       "      <td>addicted</td>\n",
       "      <td>rt @USER_______: @USE when are you going to do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>SMM4H20220LbDv7WiGBW7nQR4</td>\n",
       "      <td>ADE</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>gained 30 lbs</td>\n",
       "      <td>I might have gained 30 lbs in 6 weeks on Zypre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_id class  begin  end                  span  \\\n",
       "0   SMM4H2022ykI8vN7jZYnV57AM   ADE    119  125                nerves   \n",
       "1   SMM4H2022ykI8vN7jZYnV57AM   ADE    126  139         muscle spasms   \n",
       "2   SMM4H2022uCZV2SRsCe4vzjFm   ADE     61   68               gaining   \n",
       "3   SMM4H2022uCZV2SRsCe4vzjFm   ADE     91  110   gain like 50 pounds   \n",
       "4   SMM4H20229Aha6m4XERqYdFWf   ADE    118  134      frontal headache   \n",
       "..                        ...   ...    ...  ...                   ...   \n",
       "82  SMM4H2022cjwbGQbnkpVjjJzR   ADE     27   39          cotton mouth   \n",
       "83  SMM4H2022lX1A8RVuySkDzAB2   ADE     84  104  couldn't fall asleep   \n",
       "84  SMM4H2022qZC2BPG2BW7UC175   ADE     45   48                   OCD   \n",
       "85  SMM4H20226b5WyZPTAJ7qL3dE   ADE     63   71              addicted   \n",
       "86  SMM4H20220LbDv7WiGBW7nQR4   ADE     13   26         gained 30 lbs   \n",
       "\n",
       "                                                tweet  \n",
       "0   @USER_________ i found the humira to fix all m...  \n",
       "1   @USER_________ i found the humira to fix all m...  \n",
       "2   @USER__________ have to go to a doc now to see...  \n",
       "3   @USER__________ have to go to a doc now to see...  \n",
       "4   06.30 day 14 Rivaroxaban diary. Thanks to para...  \n",
       "..                                                ...  \n",
       "82      Fucking Vyvanse, giving me cotton mouth. UGH.  \n",
       "83  between the fucking redbull and vyvanse i popp...  \n",
       "84  rt @USER_______: vyvanse, commonly known as oc...  \n",
       "85  rt @USER_______: @USE when are you going to do...  \n",
       "86  I might have gained 30 lbs in 6 weeks on Zypre...  \n",
       "\n",
       "[87 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df = pd.read_csv('Datasets/Subtask_1b/training/dev.tsv', sep='\\t')\n",
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           id  \\\n",
      "0   SMM4H2022ykI8vN7jZYnV57AM   \n",
      "1   SMM4H2022ykI8vN7jZYnV57AM   \n",
      "2   SMM4H2022uCZV2SRsCe4vzjFm   \n",
      "3   SMM4H2022uCZV2SRsCe4vzjFm   \n",
      "4   SMM4H20229Aha6m4XERqYdFWf   \n",
      "..                        ...   \n",
      "82  SMM4H2022cjwbGQbnkpVjjJzR   \n",
      "83  SMM4H2022lX1A8RVuySkDzAB2   \n",
      "84  SMM4H2022qZC2BPG2BW7UC175   \n",
      "85  SMM4H20226b5WyZPTAJ7qL3dE   \n",
      "86  SMM4H20220LbDv7WiGBW7nQR4   \n",
      "\n",
      "                                                 text orig_begin orig_end  \\\n",
      "0   @USER_________ i found the humira to fix all m...        119      125   \n",
      "1   @USER_________ i found the humira to fix all m...        126      139   \n",
      "2   @USER__________ have to go to a doc now to see...         61       68   \n",
      "3   @USER__________ have to go to a doc now to see...         91      110   \n",
      "4   06.30 day 14 Rivaroxaban diary. Thanks to para...        118      134   \n",
      "..                                                ...        ...      ...   \n",
      "82      Fucking Vyvanse, giving me cotton mouth. UGH.         27       39   \n",
      "83  between the fucking redbull and vyvanse i popp...         84      104   \n",
      "84  rt @USER_______: vyvanse, commonly known as oc...         45       48   \n",
      "85  rt @USER_______: @USE when are you going to do...         63       71   \n",
      "86  I might have gained 30 lbs in 6 weeks on Zypre...         13       26   \n",
      "\n",
      "               orig_span     orig_span_convert  \\\n",
      "0                 nerves                nerves   \n",
      "1          muscle spasms         muscle spasms   \n",
      "2                gaining               gaining   \n",
      "3    gain like 50 pounds   gain like 50 pounds   \n",
      "4       frontal headache      frontal headache   \n",
      "..                   ...                   ...   \n",
      "82          cotton mouth          cotton mouth   \n",
      "83  couldn't fall asleep  couldn't fall asleep   \n",
      "84                   OCD                   ocd   \n",
      "85              addicted              addicted   \n",
      "86         gained 30 lbs         gained 30 lbs   \n",
      "\n",
      "                                         predict_span predict_begin  \\\n",
      "0                                nerves/muscle spasms           118   \n",
      "1                                nerves/muscle spasms           118   \n",
      "2   gaining. stupid paxil made me gain like 50 pounds            61   \n",
      "3   gaining. stupid paxil made me gain like 50 pounds            61   \n",
      "4                                            headache           126   \n",
      "..                                                ...           ...   \n",
      "82                                       cotton mouth            27   \n",
      "83                                        fall asleep            93   \n",
      "84                               ommonly known as ocd            27   \n",
      "85                                           addicted            63   \n",
      "86  gained 30 lbs in 6 weeks on Zyprexa...Haven't ...            13   \n",
      "\n",
      "   predict_end  \n",
      "0          138  \n",
      "1          138  \n",
      "2          110  \n",
      "3          110  \n",
      "4          134  \n",
      "..         ...  \n",
      "82          39  \n",
      "83         104  \n",
      "84          47  \n",
      "85          71  \n",
      "86          75  \n",
      "\n",
      "[87 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "best_results = []\n",
    "\n",
    "for index, row in original_df.iterrows():\n",
    "    # getting the original values in the tweet (to minimize shuffling)\n",
    "    original_tweet_id = row['tweet_id']\n",
    "    original_begin = row['begin']\n",
    "    original_end = row['end']\n",
    "    \n",
    "    # go through every epoch's row of data \n",
    "    specific_row_result = []\n",
    "    for epoch in range(len(list_of_df)): \n",
    "        row = list_of_df[epoch].loc[(list_of_df[epoch]['id'] == original_tweet_id) & (list_of_df[epoch]['orig_begin'] == original_begin) & (list_of_df[epoch]['orig_end'] == original_end)]\n",
    "        # print(row)\n",
    "        # retrieving the length of the span\n",
    "        predicted_begin = row['predict_begin'].values[0]\n",
    "        predicted_end = row['predict_end'].values[0]\n",
    "        span_length = predicted_end - predicted_begin\n",
    "        \n",
    "        if span_length == 0:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        specific_row_result.append((epoch, row['Unnamed: 0'].values[0], span_length))\n",
    "    \n",
    "    # sort the specific_row_results by increasing span length\n",
    "    specific_row_result.sort(key = lambda x : x[2])\n",
    "    best_results.append(specific_row_result)\n",
    "\n",
    "# go through every specific_row_results and keeping the median value ONLY\n",
    "for row in range(len(best_results)):\n",
    "    \n",
    "    length_specific_result = len(best_results[row])\n",
    "    \n",
    "    if length_specific_result == 0: \n",
    "        continue\n",
    "\n",
    "    median_of_length_specific_result = int(length_specific_result / 2)\n",
    "\n",
    "    best_results[row] = best_results[row][median_of_length_specific_result]\n",
    "\n",
    "\n",
    "# appending all the results to create one final result\n",
    "unformatted_merged_result = pd.DataFrame(columns = ['id', 'text', 'orig_begin', 'orig_end', 'orig_span', 'orig_span_convert', 'predict_span', 'predict_begin', 'predict_end'])\n",
    "\n",
    "for row in range(len(best_results)):\n",
    "    if best_results[row] == []:\n",
    "        # retrieving it from the original dataframe\n",
    "        original_row_result = original_df.iloc[row]\n",
    "        tweet_id = original_row_result['tweet_id']\n",
    "        orig_sentence = original_row_result['tweet']\n",
    "        orig_begin = original_row_result['begin']\n",
    "        orig_end = original_row_result['end']\n",
    "        orig_span = original_row_result['span']\n",
    "        len_orig_sentence = len(orig_sentence)\n",
    "        \n",
    "        # creating a dataframe row and adding the lengths of the original sentences to the result\n",
    "        \n",
    "        to_append = pd.DataFrame({'id': [tweet_id], 'text': [orig_sentence], 'orig_begin': [orig_begin], 'orig_end': [orig_end], 'orig_span': [orig_span], 'orig_span_convert': [\"No guess\"], 'predict_span': [orig_sentence], 'predict_begin': [0], 'predict_end': [len_orig_sentence]})\n",
    "        unformatted_merged_result = unformatted_merged_result.append(to_append, ignore_index=True)\n",
    "\n",
    "        continue\n",
    "    \n",
    "    original_row_result = list_of_df[best_results[row][0]].iloc[best_results[row][1]]\n",
    "    tweet_id = original_row_result['id']\n",
    "    orig_sentence = original_row_result['text']\n",
    "    orig_begin = original_row_result['orig_begin']\n",
    "    orig_end = original_row_result['orig_end']\n",
    "    orig_span = original_row_result['orig_span']\n",
    "    orig_span_convert = original_row_result['orig_span_convert']\n",
    "    predict_span = original_row_result['predict_span']\n",
    "    predict_begin = original_row_result['predict_begin']\n",
    "    predict_end = original_row_result['predict_end']\n",
    "\n",
    "    to_append = pd.DataFrame({'id': [tweet_id], 'text': [orig_sentence], 'orig_begin': [orig_begin], 'orig_end': [orig_end], 'orig_span': [orig_span], 'orig_span_convert': [orig_span_convert], 'predict_span': [predict_span], 'predict_begin': [predict_begin], 'predict_end': [predict_end]})\n",
    "    unformatted_merged_result = unformatted_merged_result.append(to_append, ignore_index=True)\n",
    "\n",
    "    \n",
    "print(unformatted_merged_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the result\n",
    "formatted_merged_result = unformatted_merged_result\n",
    "\n",
    "unformatted_merged_result.to_csv('bert-base/20_epochs_baseline/unformatted_merged_result.tsv', sep='\\t', index=False)\n",
    "formatted_merged_result.to_csv('bert-base/20_epochs_baseline/formatted_merged_reuslt.tsv', sep='\\t', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
